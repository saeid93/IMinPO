{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import ndlib.models.ModelConfig as mc\n",
    "import ndlib.models.epidemics.IndependentCascadesModel as ids\n",
    "import ndlib.models.epidemics.ThresholdModel as th\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"CA-HepTh-2\"\n",
    "numOfNodes = 9877\n",
    "f = 0.8 # percentage to trim\n",
    "diffP = 0.5\n",
    "sizeOfSeed = 100\n",
    "# number of generated graphs\n",
    "numOfSim1 = 1000\n",
    "# --------- IM algorithms prameteres ---------\n",
    "# Static Greedy\n",
    "R = 200\n",
    "# IM rank\n",
    "L = 1\n",
    "iters = 10\n",
    "# --------- Diff test parameteres ---------\n",
    "# Type of IM seed\n",
    "numOfSims = 3\n",
    "numOfIters = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ID Checker\n",
    "***checks the ids of the input graph***<br/>\n",
    "This script checks that if the IDs of the node start with 1 and end in the number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjList = np.loadtxt(\"Data/1 Raw Datasets/input.csv\", delimiter=',')\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.sort(np.unique(adjList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ID fixer\n",
    "***Fix IDs if the previous step indicate they are corrupted***<br/>\n",
    "The IDs are from 1 to number of nodes after this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjList1 = np.loadtxt(\"Data/1 Raw Datasets/input.csv\", delimiter=',')\n",
    "\n",
    "allNodes = np.sort(np.unique(adjList1))\n",
    "numOfNodes = np.shape(allNodes)[0]\n",
    "for i in range(0,numOfNodes):\n",
    "    np.place(adjList1,adjList1==allNodes[i],i+1)\n",
    "\n",
    "df = pd.DataFrame(adjList1)\n",
    "df = df.astype(np.int64)\n",
    "\n",
    "df.to_csv(\"Data/2 Fixed Id datasets/input.csv\",header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reducer\n",
    "***Trim a percent of the edges of the graph***<br/>\n",
    "Delete a number of edges and save all the reduced graphs duplicated and non-duplicated versions<br/>\n",
    "***--> Be careful with every run of this script a new set of nodes are deleted <--***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(\"Data/2 Fixed Id datasets/input.csv\").values\n",
    "numOfEdges = np.shape(original)[0]\n",
    "\n",
    "\n",
    "# make the non-duplicated version of the input original graph\n",
    "without_repeat = set()\n",
    "for e in original:\n",
    "    if (e[0], e[1]) in without_repeat or (e[1], e[0]) in without_repeat:\n",
    "        pass\n",
    "    else:\n",
    "        without_repeat.add((e[0], e[1]))\n",
    "\n",
    "\n",
    "# convert set to numpy array\n",
    "# ***it is the non-dulicated version***\n",
    "original = []\n",
    "for e in without_repeat:\n",
    "    original.append(list(e))\n",
    "original = np.array(original)\n",
    "\n",
    "\n",
    "# trim the desired percentage of the nodes\n",
    "size = len(original)\n",
    "idx = np.random.choice(np.arange(size), int(f*size), replace=False)\n",
    "deleted = np.array(list(set(np.arange(size)) - set(idx)))\n",
    "\n",
    "reducedGraph = original[idx]\n",
    "deletedEdges = original[deleted]\n",
    "\n",
    "\n",
    "# save the reduced graphs and deleted edges graph non-duplicated version\n",
    "np.savetxt(\"Data/3 Reduced/reducedGraph.csv\",reducedGraph, fmt='%d',delimiter=',')\n",
    "np.savetxt(\"Data/3 Reduced/deletedEdges.csv\",deletedEdges, fmt='%d',delimiter=',')\n",
    "# build the reduced graphs and deleted edges graph duplicated version\n",
    "reducedGraphDup = np.vstack([reducedGraph[:, [1,0]], reducedGraph])\n",
    "deletedEdgesDup = np.vstack([deletedEdges[:, [1,0]], deletedEdges])\n",
    "# save the reduced graphs and deleted edges graph duplicated version\n",
    "np.savetxt(\"Data/3 Reduced/reducedGraphDup.csv\",reducedGraphDup, fmt='%d',delimiter=',')\n",
    "np.savetxt(\"Data/3 Reduced/deletedEdgesDup.csv\",deletedEdgesDup, fmt='%d',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Read Adjcency list to R\n",
    "***4 Read the R adj list into an R model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: tergm\n",
      "Loading required package: ergm\n",
      "Loading required package: network\n",
      "network: Classes for Relational Data\n",
      "Version 1.13.0.1 created on 2015-08-31.\n",
      "copyright (c) 2005, Carter T. Butts, University of California-Irvine\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Martina Morris, University of Washington\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      " For citation information, type citation(\"network\").\n",
      " Type help(\"network-package\") to get started.\n",
      "\n",
      "\n",
      "ergm: version 3.9.4, created on 2018-08-15\n",
      "Copyright (c) 2018, Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Martina Morris, University of Washington\n",
      "                    with contributions from\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"ergm\").\n",
      "\n",
      "NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()\n",
      "constriant which distorted the sampled distribution somewhat. In\n",
      "addition, Sampson's Monks datasets had mislabeled vertices. See the\n",
      "NEWS and the documentation for more details.\n",
      "\n",
      "Loading required package: networkDynamic\n",
      "\n",
      "networkDynamic: version 0.9.0, created on 2016-01-12\n",
      "Copyright (c) 2016, Carter T. Butts, University of California -- Irvine\n",
      "                    Ayn Leslie-Cook, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      "                    with contributions from\n",
      "                    Zack Almquist, University of California -- Irvine\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Jeffrey Horner\n",
      "                    Martina Morris, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"networkDynamic\").\n",
      "\n",
      "\n",
      "tergm: version 3.5.2, created on 2018-08-18\n",
      "Copyright (c) 2018, Pavel N. Krivitsky, University of Wollongong\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    with contributions from\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Martina Morris, University of Washington\n",
      "                    Nicole Bohme Carnegie, New York University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Ayn Leslie-Cook, University of Washington\n",
      "                    Skye Bender-deMoll\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"tergm\").\n",
      "\n",
      "Loading required package: ergm.count\n",
      "Loading required package: statnet.common\n",
      "\n",
      "Attaching package: ‘statnet.common’\n",
      "\n",
      "The following objects are masked from ‘package:ergm’:\n",
      "\n",
      "    colMeans.mcmc.list, sweep.mcmc.list\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    order\n",
      "\n",
      "\n",
      "ergm.count: version 3.3.0, created on 2018-08-25\n",
      "Copyright (c) 2018, Pavel N. Krivitsky, University of Wollongong\n",
      "                    with contributions from\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"ergm.count\").\n",
      "\n",
      "NOTE: The form of the term ‘CMP’ has been changed in version 3.2 of\n",
      "‘ergm.count’. See the news or help('CMP') for more information.\n",
      "\n",
      "Loading required package: sna\n",
      "sna: Tools for Social Network Analysis\n",
      "Version 2.4 created on 2016-07-23.\n",
      "copyright (c) 2005, Carter T. Butts, University of California-Irvine\n",
      " For citation information, type citation(\"sna\").\n",
      " Type help(package=\"sna\") to get started.\n",
      "\n",
      "Loading required package: tsna\n",
      "\n",
      "statnet: version 2018.10, created on 2018-10-17\n",
      "Copyright (c) 2018, Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Skye Bender-deMoll\n",
      "                    Martina Morris, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"statnet\").\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!Rscript 4\\ Read\\ R\\ Adj\\ Input.R {numOfNodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 ERGM Model Builder\n",
    "***5 build the ERGM model from the adj list***<br/>\n",
    "***--> Be careful with every run of this script a new model is built<--***<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"This script is in charge of building the \\n  ergm model\"\n",
      "Loading required package: tergm\n",
      "Loading required package: ergm\n",
      "Loading required package: network\n",
      "network: Classes for Relational Data\n",
      "Version 1.13.0.1 created on 2015-08-31.\n",
      "copyright (c) 2005, Carter T. Butts, University of California-Irvine\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Martina Morris, University of Washington\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      " For citation information, type citation(\"network\").\n",
      " Type help(\"network-package\") to get started.\n",
      "\n",
      "\n",
      "ergm: version 3.9.4, created on 2018-08-15\n",
      "Copyright (c) 2018, Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Martina Morris, University of Washington\n",
      "                    with contributions from\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"ergm\").\n",
      "\n",
      "NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()\n",
      "constriant which distorted the sampled distribution somewhat. In\n",
      "addition, Sampson's Monks datasets had mislabeled vertices. See the\n",
      "NEWS and the documentation for more details.\n",
      "\n",
      "Loading required package: networkDynamic\n",
      "\n",
      "networkDynamic: version 0.9.0, created on 2016-01-12\n",
      "Copyright (c) 2016, Carter T. Butts, University of California -- Irvine\n",
      "                    Ayn Leslie-Cook, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      "                    with contributions from\n",
      "                    Zack Almquist, University of California -- Irvine\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Jeffrey Horner\n",
      "                    Martina Morris, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"networkDynamic\").\n",
      "\n",
      "\n",
      "tergm: version 3.5.2, created on 2018-08-18\n",
      "Copyright (c) 2018, Pavel N. Krivitsky, University of Wollongong\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    with contributions from\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Martina Morris, University of Washington\n",
      "                    Nicole Bohme Carnegie, New York University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Ayn Leslie-Cook, University of Washington\n",
      "                    Skye Bender-deMoll\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"tergm\").\n",
      "\n",
      "Loading required package: ergm.count\n",
      "Loading required package: statnet.common\n",
      "\n",
      "Attaching package: ‘statnet.common’\n",
      "\n",
      "The following objects are masked from ‘package:ergm’:\n",
      "\n",
      "    colMeans.mcmc.list, sweep.mcmc.list\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    order\n",
      "\n",
      "\n",
      "ergm.count: version 3.3.0, created on 2018-08-25\n",
      "Copyright (c) 2018, Pavel N. Krivitsky, University of Wollongong\n",
      "                    with contributions from\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"ergm.count\").\n",
      "\n",
      "NOTE: The form of the term ‘CMP’ has been changed in version 3.2 of\n",
      "‘ergm.count’. See the news or help('CMP') for more information.\n",
      "\n",
      "Loading required package: sna\n",
      "sna: Tools for Social Network Analysis\n",
      "Version 2.4 created on 2016-07-23.\n",
      "copyright (c) 2005, Carter T. Butts, University of California-Irvine\n",
      " For citation information, type citation(\"sna\").\n",
      " Type help(package=\"sna\") to get started.\n",
      "\n",
      "Loading required package: tsna\n",
      "\n",
      "statnet: version 2018.10, created on 2018-10-17\n",
      "Copyright (c) 2018, Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Skye Bender-deMoll\n",
      "                    Martina Morris, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"statnet\").\n",
      "\n",
      "[1] \"Adjacency list read successfully!\"\n",
      "^C\n",
      "\n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "!Rscript 5\\ Build\\ ERGM\\ Model.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate Random Networks\n",
    "***6 generate simulated network from the ERGM Model***<br/>\n",
    "***--> Be careful with every run of this script a new set of graphs are generated<--***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"This script is in charge of making \\n  doing the ergm simulations\"\n",
      "Loading required package: tergm\n",
      "Loading required package: ergm\n",
      "Loading required package: network\n",
      "network: Classes for Relational Data\n",
      "Version 1.13.0.1 created on 2015-08-31.\n",
      "copyright (c) 2005, Carter T. Butts, University of California-Irvine\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Martina Morris, University of Washington\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      " For citation information, type citation(\"network\").\n",
      " Type help(\"network-package\") to get started.\n",
      "\n",
      "\n",
      "ergm: version 3.9.4, created on 2018-08-15\n",
      "Copyright (c) 2018, Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Martina Morris, University of Washington\n",
      "                    with contributions from\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"ergm\").\n",
      "\n",
      "NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()\n",
      "constriant which distorted the sampled distribution somewhat. In\n",
      "addition, Sampson's Monks datasets had mislabeled vertices. See the\n",
      "NEWS and the documentation for more details.\n",
      "\n",
      "Loading required package: networkDynamic\n",
      "\n",
      "networkDynamic: version 0.9.0, created on 2016-01-12\n",
      "Copyright (c) 2016, Carter T. Butts, University of California -- Irvine\n",
      "                    Ayn Leslie-Cook, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Skye Bender-deMoll, University of Washington\n",
      "                    with contributions from\n",
      "                    Zack Almquist, University of California -- Irvine\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Jeffrey Horner\n",
      "                    Martina Morris, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"networkDynamic\").\n",
      "\n",
      "\n",
      "tergm: version 3.5.2, created on 2018-08-18\n",
      "Copyright (c) 2018, Pavel N. Krivitsky, University of Wollongong\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    with contributions from\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Martina Morris, University of Washington\n",
      "                    Nicole Bohme Carnegie, New York University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Ayn Leslie-Cook, University of Washington\n",
      "                    Skye Bender-deMoll\n",
      "                    Li Wang\n",
      "                    Kirk Li, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"tergm\").\n",
      "\n",
      "Loading required package: ergm.count\n",
      "Loading required package: statnet.common\n",
      "\n",
      "Attaching package: ‘statnet.common’\n",
      "\n",
      "The following objects are masked from ‘package:ergm’:\n",
      "\n",
      "    colMeans.mcmc.list, sweep.mcmc.list\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    order\n",
      "\n",
      "\n",
      "ergm.count: version 3.3.0, created on 2018-08-25\n",
      "Copyright (c) 2018, Pavel N. Krivitsky, University of Wollongong\n",
      "                    with contributions from\n",
      "                    Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"ergm.count\").\n",
      "\n",
      "NOTE: The form of the term ‘CMP’ has been changed in version 3.2 of\n",
      "‘ergm.count’. See the news or help('CMP') for more information.\n",
      "\n",
      "Loading required package: sna\n",
      "sna: Tools for Social Network Analysis\n",
      "Version 2.4 created on 2016-07-23.\n",
      "copyright (c) 2005, Carter T. Butts, University of California-Irvine\n",
      " For citation information, type citation(\"sna\").\n",
      " Type help(package=\"sna\") to get started.\n",
      "\n",
      "Loading required package: tsna\n",
      "\n",
      "statnet: version 2018.10, created on 2018-10-17\n",
      "Copyright (c) 2018, Mark S. Handcock, University of California -- Los Angeles\n",
      "                    David R. Hunter, Penn State University\n",
      "                    Carter T. Butts, University of California -- Irvine\n",
      "                    Steven M. Goodreau, University of Washington\n",
      "                    Pavel N. Krivitsky, University of Wollongong\n",
      "                    Skye Bender-deMoll\n",
      "                    Martina Morris, University of Washington\n",
      "Based on \"statnet\" project software (statnet.org).\n",
      "For license and citation information see statnet.org/attribution\n",
      "or type citation(\"statnet\").\n",
      "\n",
      "[1] \"model loaded successfully!\"\n",
      "^C\n",
      "\n",
      "Warning message:\n",
      "You appear to be calling simulate.formula() directly. simulate.formula() is a method, and will not be exported in a future version of ‘ergm’. Use simulate() instead, or getS3method() if absolutely necessary. \n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "!Rscript 6\\ Simulating\\ Similar\\ Networks.R {numOfSim1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Link Prediction\n",
    "***generate link predicted network***<br/>\n",
    "average over all of them<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of added edge to the reduced graph: 5200\n",
      "The difference between actual deleted edges and the added ones is: 3748\n"
     ]
    }
   ],
   "source": [
    "# list the files in the simulated graphs directory\n",
    "files  = os.listdir(\"Data/6 Simulated Networks/\")\n",
    "adjLists = [] #containing whole 1000 generated graphs\n",
    "\n",
    "# read the grpahs adjacency lists\n",
    "for file in files:\n",
    "    adjLists = adjLists + [np.genfromtxt(\"Data/6 Simulated Networks/\"+file, delimiter=\",\",dtype=np.int64)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### count the number of each edge repeat in the whole simulated nets set\n",
    "c = defaultdict(int)\n",
    "for adj in tqdm(adjLists):\n",
    "    ad1 = map(lambda x: (x[0], x[1]), adj)\n",
    "    for i in ad1:\n",
    "        c[i] += 1\n",
    "    ad1 = map(lambda x: (x[1], x[0]), adj)\n",
    "    for i in ad1:\n",
    "        c[i] += 1\n",
    "\n",
    "\n",
    "# ### read the reduced and deleted edges from their file\n",
    "# (***this files are with duplicate edges***)\n",
    "reduced = np.genfromtxt(\"Data/3 Reduced/reducedGraphDup.csv\", delimiter=\",\",dtype=np.int64)\n",
    "deleted1 = np.genfromtxt(\"Data/3 Reduced/deletedEdgesDup.csv\", delimiter=\",\",dtype=np.int64)\n",
    "\n",
    "# ### sorted array to find the top most repeated edges in the simulated networks\n",
    "# ***we aim to add most repeated with the same number of deleted edges***\n",
    "# ***in most repeated there are duplicate edges***\n",
    "most_repeated = sorted(c, key=lambda x: c[x], reverse=True)\n",
    "\n",
    "\n",
    "# convert numpy array to set\n",
    "reduced_set = set(map(lambda x: (x[0], x[1]), reduced))\n",
    "\n",
    "to_add = set()\n",
    "size1 = deleted1.shape[0]//2\n",
    "for e in most_repeated:\n",
    "    if not e in reduced_set:\n",
    "        if e in to_add or (e[1], e[0]) in to_add or e[0] > numOfNodes or e[1] > numOfNodes:\n",
    "            continue\n",
    "        to_add.add(e)\n",
    "        size1 -= 1\n",
    "        if size1 == 0:\n",
    "            break\n",
    "print(\"The number of added edge to the reduced graph: \"+ str(len(to_add)) )\n",
    "\n",
    "\n",
    "# ### build the nonDuplicated verstion of reduced\n",
    "reducedNonDup = set()\n",
    "for e in reduced_set:\n",
    "    if (e in reducedNonDup) or ((e[1], e[0]) in reducedNonDup):\n",
    "        pass\n",
    "    else:\n",
    "        reducedNonDup |= {e}\n",
    "\n",
    "\n",
    "# ### adding the added edges to reduced graph\n",
    "reducedNonDup.update(to_add)\n",
    "\n",
    "actual_added = to_add - set(map(lambda x: (x[0], x[1]), deleted1))\n",
    "\n",
    "print(\"The difference between actual deleted edges and the added ones is: \" + str(len(actual_added)))\n",
    "\n",
    "# ### to remeber\n",
    "# ***reducedNonDup: is the graph with added edges and it is obviously non-dup***\n",
    "# ***to_add: is non-duplicated***\n",
    "# ***actual_added: is non-duplicated***\n",
    "\n",
    "def save_edge_set(fname, edge_set):\n",
    "    with open(fname, 'w') as of:\n",
    "        for e in edge_set:\n",
    "            of.write(\"%d,%d\\n\"%e)\n",
    "\n",
    "save_edge_set(\"Data/7 Link Predicted Network/augmented.csv\",reducedNonDup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Non-duplicated Builder\n",
    "***generate non-duplicated version of the original graph***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51971/51971 [00:00<00:00, 997936.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_edge_set(fname, edge_set):\n",
    "    with open(fname, 'w') as of:\n",
    "        for e in edge_set:\n",
    "            of.write(\"%d,%d\\n\"%e)\n",
    "\n",
    "\n",
    "duplicated = np.genfromtxt(\"Data/2 Fixed Id datasets/input.csv\", delimiter=\",\",dtype=np.int64)\n",
    "duplicated_set = set(map(lambda x: (x[0], x[1]), duplicated))\n",
    "\n",
    "nonDup = set()\n",
    "for e in tqdm(duplicated_set):\n",
    "    if (e in nonDup) or ((e[1], e[0]) in nonDup):\n",
    "        pass\n",
    "    else:\n",
    "        nonDup |= {e}\n",
    "\n",
    "        \n",
    "save_edge_set(\"Data/8 Original Without Duplicates/original.csv\",nonDup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Random Graph Builder\n",
    "***build a random graph with the same size as the augmented and the original graph***<br/>\n",
    "***--> Be careful with every run of this script a new set of graphs are generated<--***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomEdge(numOfNodes):\n",
    "    return (np.random.randint(1,numOfNodes+1),random.randint(1,numOfNodes+1))\n",
    "\n",
    "def save_edge_set(fname, edge_set):\n",
    "    with open(fname, 'w') as of:\n",
    "        for e in edge_set:\n",
    "            of.write(\"%d,%d\\n\"%e)\n",
    "\n",
    "\n",
    "reduced1 = np.genfromtxt(\"Data/3 Reduced/reducedGraph.csv\",delimiter=',',dtype=int)\n",
    "nOfNs2Gen = len(np.genfromtxt(\"Data/3 Reduced/deletedEdges.csv\",delimiter=',',dtype=int))\n",
    "\n",
    "\n",
    "reduced_set1 = set(map(lambda x : (x[0],x[1]),reduced1))\n",
    "\n",
    "size2 = nOfNs2Gen\n",
    "while size2:\n",
    "    e = randomEdge(numOfNodes)\n",
    "    if (not e in reduced_set1) and (not (e[1],e[0]) in reduced_set1):\n",
    "        reduced_set1.add(e)\n",
    "        size2 -=1\n",
    "\n",
    "\n",
    "save_edge_set(\"Data/9 Random Graph/random.csv\",reduced_set1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Diffusion Added\n",
    "***Add diffusion probability to the graphs*** <br/>\n",
    "The three augmented, random and original without duplicates are built at this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = np.genfromtxt(\"Data/7 Link Predicted Network/augmented.csv\",delimiter=',',dtype = int)\n",
    "original1 = np.genfromtxt(\"Data/8 Original Without Duplicates/original.csv\",delimiter=',',dtype = int)\n",
    "random1 = np.genfromtxt(\"Data/9 Random Graph/random.csv\",delimiter=',',dtype = int)\n",
    "\n",
    "\n",
    "def addProbs(adj,prefix):\n",
    "    with open(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/\" + prefix + \".csv\",'w') as of:\n",
    "        for x,y in adj:\n",
    "            of.write(\"%d\\t%d\\t%.1f\\n\"%(x,y,diffP))\n",
    "    with open(\"Data/10 IM ready Graphs with Diffusion Probabilities/IM/\" + prefix + \".csv\",'w') as of:\n",
    "        of.write(\"%d\\t%d\\n\"%(numOfNodes,len(adj)))\n",
    "        for x,y in adj:\n",
    "            of.write(\"%d\\t%d\\t%.1f\\n\"%(x,y,diffP))\n",
    "            \n",
    "\n",
    "addProbs(augmented,\"augmented\")\n",
    "addProbs(original1,\"original\")\n",
    "addProbs(random1,\"random\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1 IM (Centralities)\n",
    "***compute centralities of the graphs***<br/>\n",
    "Betweenness and PageRank centralities are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intt(x):\n",
    "    return int(float(x))\n",
    "\n",
    "def write2file(file,seed):\n",
    "    f= open(file,\"w+\")\n",
    "    for item in seed:\n",
    "         f.write(str(item) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "augmented1 = nx.read_weighted_edgelist(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/augmented.csv\",nodetype=intt)\n",
    "original2 = nx.read_weighted_edgelist(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/original.csv\",nodetype=intt)\n",
    "random2 = nx.read_weighted_edgelist(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/random.csv\",nodetype=intt)\n",
    "\n",
    "def computeCentrality(g , graphType):\n",
    "#     betweenness centrality\n",
    "    BetCen = nx.betweenness_centrality(g)\n",
    "    BetCenSeed = sorted(BetCen.items(), key=itemgetter(1), reverse=True)[:sizeOfSeed]\n",
    "    BetCenSeed = list(map(lambda x: x[0], BetCenSeed))\n",
    "    write2file(\"Data/11 IM stage results/Betweenness Centrality/\" + graphType + \".csv\",BetCenSeed)\n",
    "#     PageRank Centralilty\n",
    "    PRCen = nx.pagerank(g)\n",
    "    PRCenSeed = sorted(PRCen.items(), key=itemgetter(1), reverse=True)[:sizeOfSeed]\n",
    "    PRCenSeed = list(map(lambda x: x[0], PRCenSeed))\n",
    "    write2file(\"Data/11 IM stage results/PageRank Centrality/\" + graphType + \".csv\",PRCenSeed)\n",
    "\n",
    "    \n",
    "computeCentrality(augmented1,\"augmented\")\n",
    "computeCentrality(original2,\"original\")\n",
    "computeCentrality(random2,\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2. random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def computeRandomSeed(graphType):\n",
    "randSeed = np.random.randint(1,numOfNodes,(sizeOfSeed,1))\n",
    "np.savetxt(\"Data/11 IM stage results/Random/original.csv\",randSeed,fmt=\"%d\")\n",
    "np.savetxt(\"Data/11 IM stage results/Random/augmented.csv\",randSeed,fmt=\"%d\")\n",
    "np.savetxt(\"Data/11 IM stage results/Random/random.csv\",randSeed,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2. IM (IMs)\n",
    "***compute results of the IM methods***<br/>\n",
    "IMrank and Static Greedy methods have been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network info:9877  25998\r\n"
     ]
    }
   ],
   "source": [
    "!./Static\\ Greedy Data/10\\ IM\\ ready\\ Graphs\\ with\\ Diffusion\\ Probabilities/IM/augmented.csv {sizeOfSeed} {R} bsg\n",
    "!tail -n+5 BasicStaticGreedy_R{R}_k{sizeOfSeed}.txt > Data/11\\ IM\\ stage\\ results/Static\\ Greedy/augmented.csv\n",
    "!rm BasicStaticGreedy_R{R}_k{sizeOfSeed}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network info:9877  25998\r\n"
     ]
    }
   ],
   "source": [
    "!./Static\\ Greedy Data/10\\ IM\\ ready\\ Graphs\\ with\\ Diffusion\\ Probabilities/IM/original.csv {sizeOfSeed} {R} bsg\n",
    "!tail -n+5 BasicStaticGreedy_R{R}_k{sizeOfSeed}.txt > Data/11\\ IM\\ stage\\ results/Static\\ Greedy/original.csv\n",
    "!rm BasicStaticGreedy_R{R}_k{sizeOfSeed}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network info:9877  25998\r\n"
     ]
    }
   ],
   "source": [
    "!./Static\\ Greedy Data/10\\ IM\\ ready\\ Graphs\\ with\\ Diffusion\\ Probabilities/IM/random.csv {sizeOfSeed} {R} bsg\n",
    "!tail -n+5 BasicStaticGreedy_R{R}_k{sizeOfSeed}.txt > Data/11\\ IM\\ stage\\ results/Static\\ Greedy/random.csv\n",
    "!rm BasicStaticGreedy_R{R}_k{sizeOfSeed}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network info:9877  25998\r\n"
     ]
    }
   ],
   "source": [
    "!./IMrank Data/10\\ IM\\ ready\\ Graphs\\ with\\ Diffusion\\ Probabilities/IM/original.csv {sizeOfSeed} {L} {iters} PageRank\n",
    "!tail -n+5 IMRank_k{sizeOfSeed}_l{L}_LOOP{iters}_irPageRank.txt > Data/11\\ IM\\ stage\\ results/IMrank/original.csv\n",
    "!rm IMRank_k{sizeOfSeed}_l{L}_LOOP{iters}_irPageRank.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network info:9877  25998\r\n"
     ]
    }
   ],
   "source": [
    "!./IMrank Data/10\\ IM\\ ready\\ Graphs\\ with\\ Diffusion\\ Probabilities/IM/augmented.csv {sizeOfSeed} {L} {iters} PageRank\n",
    "!tail -n+5 IMRank_k{sizeOfSeed}_l{L}_LOOP{iters}_irPageRank.txt > Data/11\\ IM\\ stage\\ results/IMrank/augmented.csv\n",
    "!rm IMRank_k{sizeOfSeed}_l{L}_LOOP{iters}_irPageRank.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network info:9877  25998\r\n"
     ]
    }
   ],
   "source": [
    "!./IMrank Data/10\\ IM\\ ready\\ Graphs\\ with\\ Diffusion\\ Probabilities/IM/random.csv {sizeOfSeed} {L} {iters} PageRank\n",
    "!tail -n+5 IMRank_k{sizeOfSeed}_l{L}_LOOP{iters}_irPageRank.txt > Data/11\\ IM\\ stage\\ results/IMrank/random.csv\n",
    "!rm IMRank_k{sizeOfSeed}_l{L}_LOOP{iters}_irPageRank.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Diffusion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.88s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.88s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.92s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.85s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def intt(x):\n",
    "    return int(float(x))\n",
    "\n",
    "def buildGraph(adjList,numOfNodes):\n",
    "    numOfEle = np.shape(adjList)[0]\n",
    "    g = nx.empty_graph(numOfNodes+1)\n",
    "    for i in range (0,numOfEle):\n",
    "        g.add_edge(adjList[i,0],adjList[i,1])\n",
    "    return (g)\n",
    "\n",
    "\n",
    "\n",
    "# read the graphs from file\n",
    "augmentedAdj = np.genfromtxt(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/augmented.csv\")\n",
    "originalAdj = np.genfromtxt(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/original.csv\")\n",
    "randomAdj = np.genfromtxt(\"Data/10 IM ready Graphs with Diffusion Probabilities/Cenrality/random.csv\")\n",
    "\n",
    "# build the graphs\n",
    "augmentedGraph = buildGraph(augmentedAdj,numOfNodes)\n",
    "originalGraph = buildGraph(originalAdj,numOfNodes)\n",
    "randomGraph = buildGraph(randomAdj,numOfNodes)\n",
    "\n",
    "# simulation paprameters\n",
    "# g: input graph\n",
    "# seed: input seed\n",
    "def simulation(g,seed):\n",
    "    # trend array\n",
    "    trendArr = []\n",
    "    # Model selection\n",
    "    model = ids.IndependentCascadesModel(g)\n",
    "    # Model Configuration\n",
    "    config = mc.Configuration()\n",
    "    # config.add_model_parameter\n",
    "    config.add_model_initial_configuration(\"Infected\", seed)\n",
    "    # Setting the edge parameters\n",
    "    for e in g.edges():\n",
    "        config.add_edge_configuration(\"threshold\",e , diffP)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    # Simulation execution\n",
    "#     iterations = model.iteration_bunch(numOfIters)\n",
    "        # Simulation execution\n",
    "    for j in range(0,numOfIters):\n",
    "        iterations = model.iteration_bunch(1)\n",
    "        trends = model.build_trends(iterations)\n",
    "        di = model.status\n",
    "        infectedTrend = np.array(list(filter(lambda x: di[x] in [1, 2], di)))\n",
    "        trendArr = trendArr + [infectedTrend]\n",
    "    di = model.status\n",
    "    infected = np.array(list(filter(lambda x: di[x] in [1, 2], di)))\n",
    "    return ((infected,trendArr))\n",
    "#         print('------------------------------------')\n",
    "#         print(sum(value == 0 for value in model.status.values()))\n",
    "#         print(sum(value == 1 for value in model.status.values()))\n",
    "#         print(sum(value == 2 for value in model.status.values()))\n",
    "        \n",
    "# run simulation per graphs\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self, ar_size):\n",
    "        self.sum = np.zeros(ar_size)\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, arr):\n",
    "        self.sum += arr\n",
    "        self.count += 1\n",
    "        \n",
    "    def get(self):\n",
    "        return self.sum / self.count\n",
    "    \n",
    "def tests(IMmethod):\n",
    "    # set the arrays that keeps the num of infected\n",
    "    augsAr = np.zeros(numOfSims)\n",
    "    randAr = np.zeros(numOfSims)\n",
    "    # read the seed set from the file\n",
    "    augmentedSeed = np.genfromtxt(\"Data/11 IM stage results/\" + IMmethod + \"/augmented.csv\",dtype=int)\n",
    "    originalSeed = np.genfromtxt(\"Data/11 IM stage results/\" + IMmethod + \"/original.csv\",dtype=int)\n",
    "    randomSeed = np.genfromtxt(\"Data/11 IM stage results/\" + IMmethod + \"/random.csv\",dtype=int)\n",
    "    augTrendsMean = Averager(numOfIters)\n",
    "    randTrendsMean = Averager(numOfIters)\n",
    "    for i in tqdm(range(0,numOfSims)):\n",
    "        # run simulation function\n",
    "        augmentedInfected,augTrends = simulation(augmentedGraph,augmentedSeed)\n",
    "        originalInfected,orgTrends = simulation(originalGraph,originalSeed)\n",
    "        randomInfected,randTrends = simulation(randomGraph,randomSeed)\n",
    "        augTrendsMean.update(np.array([len(set(x) and set(y)) for x, y in zip(augTrends, orgTrends)]))\n",
    "        randTrendsMean.update(np.array([len(set(x) and set(y)) for x, y in zip(orgTrends, randTrends)]))\n",
    "        # compute infected array\n",
    "        augsAr[i] = len(set(originalInfected) and set(augmentedInfected))\n",
    "        randAr[i] = len(set(originalInfected) and set(randomInfected))\n",
    "    return((augsAr,randAr, augTrendsMean.get(), randTrendsMean.get()))\n",
    "\n",
    "resSaver = open(\"Data/12 Diffusion Test Results/results.csv\", \"w\")\n",
    "def saveDifRes(IMmethod,res):\n",
    "    # save infected during iteration\n",
    "    np.save(\"Data/12 Diffusion Test Results/\" + IMmethod + \"/augmented\",res[0])\n",
    "    np.save(\"Data/12 Diffusion Test Results/\" + IMmethod + \"/random\",res[1])\n",
    "    np.save(\"Data/12 Diffusion Test Results/\" + IMmethod + \"/augmentedTrend\",res[2])\n",
    "    np.save(\"Data/12 Diffusion Test Results/\" + IMmethod + \"/randomTrend\",res[3])\n",
    "    # save infection trend\n",
    "    # ------------\n",
    "    resSaver.write(str(np.mean(res[0])) + \",\" + str(np.mean(res[1])) + \"\\n\")\n",
    "# get Betweeness Cntrality results\n",
    "res1 = tests(\"Betweenness Centrality\")\n",
    "saveDifRes(\"Betweenness Centrality\",res1)\n",
    "# get PageRank Cntrality results\n",
    "res2 = tests(\"PageRank Centrality\")\n",
    "saveDifRes(\"PageRank Centrality\",res2)\n",
    "# get IMrank Cntrality results\n",
    "res3 = tests(\"IMrank\")\n",
    "saveDifRes(\"IMrank\",res3)\n",
    "# get Static Greedy Cntrality results\n",
    "res4 = tests(\"Static Greedy\")\n",
    "saveDifRes(\"Static Greedy\",res4)\n",
    "# get random results\n",
    "res5 = tests(\"Random\")\n",
    "saveDifRes(\"Random\",res5)\n",
    "\n",
    "resSaver.close()\n",
    "\n",
    "# print(orgConvfTest)\n",
    "# print(randConvfTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"Data/12 Diffusion Test Results/Betweenness Centrality/augmentedTrend.npy\")\n",
    "b = np.load(\"Data/12 Diffusion Test Results/Betweenness Centrality/randomTrend.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 100.        , 1292.33333333, 3442.33333333, 5163.66666667,\n",
       "        6048.33333333, 6392.        , 6525.        , 6583.33333333,\n",
       "        6605.        , 6614.        , 6616.66666667, 6619.        ,\n",
       "        6620.33333333, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667,\n",
       "        6620.66666667, 6620.66666667, 6620.66666667, 6620.66666667]),\n",
       " array([ 100.        , 1150.33333333, 3230.33333333, 5328.        ,\n",
       "        6693.33333333, 7321.33333333, 7584.        , 7671.66666667,\n",
       "        7701.        , 7709.33333333, 7711.66666667, 7713.        ,\n",
       "        7713.33333333, 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ,\n",
       "        7714.        , 7714.        , 7714.        , 7714.        ]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
